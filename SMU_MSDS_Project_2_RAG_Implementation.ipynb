{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Abalone Age\n",
        "### Objective C: Retrieval Augmented Generation Implementation in Gradio\n",
        "#### DS6306: Doing Data Science\n",
        "##### Aayush Dalal & Jacqueline Vu\n",
        "##### December 10th, 2025\n"
      ],
      "metadata": {
        "id": "uTDQK1Xwae9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18RlsTrPKaNp",
        "outputId": "c2dd7a3e-2769-4062-9295-985fe4464f56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, faiss-cpu\n",
            "Successfully installed PyMuPDF-1.26.7 faiss-cpu-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8JFBEwKbyK",
        "outputId": "70eafb5e-2c52-4934-af53-28d49f885bd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQkUzJA-Kn5q",
        "outputId": "976bc8a7-7b11-4019-b895-0386dcc5d632"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import faiss\n",
        "import numpy as np\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fG36i_oLmn3",
        "outputId": "ec176d45-d6a2-4b3b-913b-0fd6a2313091"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Implementation"
      ],
      "metadata": {
        "id": "jB6GVp_s1JlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import faiss\n",
        "import numpy as np\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder path in Google Drive that needs to retrieve the PDF file from\n",
        "folder_path = '/content/drive/My Drive/PDFs/'\n",
        "\n",
        "# Step 1: Read PDF Files\n",
        "def read_pdfs(folder_path):\n",
        "    pdf_texts = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                doc = fitz.open(file_path)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((file_name, text))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "    return pdf_texts\n",
        "\n",
        "# Step 2: Chunk Text\n",
        "def chunk_text(text, chunk_size=100, overlap_sentences=2):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_sentences_for_chunk = []\n",
        "    current_word_count = 0\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_word_count = len(sentence.split())\n",
        "\n",
        "        # If adding the current sentence would significantly exceed chunk_size\n",
        "        # and we already have some sentences in the current chunk, finalize it.\n",
        "        if current_word_count > 0 and (current_word_count + sentence_word_count > chunk_size):\n",
        "            chunks.append(' '.join(current_sentences_for_chunk))\n",
        "\n",
        "            # Start a new chunk by taking 'overlap_sentences' from the end of the previous sentences\n",
        "            # Ensure we don't go out of bounds for the sentences list.\n",
        "            current_sentences_for_chunk = sentences[max(0, i - overlap_sentences) : i]\n",
        "            current_word_count = len(' '.join(current_sentences_for_chunk).split())\n",
        "\n",
        "        current_sentences_for_chunk.append(sentence)\n",
        "        current_word_count += sentence_word_count\n",
        "\n",
        "    # Add the last chunk if it's not empty\n",
        "    if current_sentences_for_chunk:\n",
        "        chunks.append(' '.join(current_sentences_for_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Create Embeddings\n",
        "def create_embeddings(text_chunks, tokenizer, model):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Step 4: Index Embeddings\n",
        "def index_embeddings(embeddings):\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Step 5: Answer Questions\n",
        "def answer_question(question, pdf_texts, index, embeddings, tokenizer, model, llm_tokenizer, llm_model, temperature, max_new_tokens, top_k=3):\n",
        "    # Create embedding for the question\n",
        "    inputs = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        question_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Search for the nearest text chunks\n",
        "    _, indices = index.search(np.array([question_embedding]), k=top_k)\n",
        "    indices = indices[0]\n",
        "\n",
        "    # Collect top-k chunks\n",
        "    retrieved_chunks = []\n",
        "    sources = []\n",
        "    # Adjust how chunks are retrieved based on chunk_mapping structure\n",
        "    all_flat_chunks = [chunk for _, chunks in pdf_texts for chunk in chunks]\n",
        "\n",
        "    for idx in indices:\n",
        "        # Determine which PDF and which chunk within that PDF the index refers to\n",
        "        current_chunk_idx = 0\n",
        "        pdf_name_for_chunk = \"\"\n",
        "        chunk_in_pdf_idx = 0\n",
        "        for pdf_name, chunks_in_pdf in pdf_texts:\n",
        "            if idx < current_chunk_idx + len(chunks_in_pdf):\n",
        "                pdf_name_for_chunk = pdf_name\n",
        "                chunk_in_pdf_idx = idx - current_chunk_idx\n",
        "                retrieved_chunks.append(chunks_in_pdf[chunk_in_pdf_idx])\n",
        "                sources.append(f\"{pdf_name_for_chunk}, Chunk {chunk_in_pdf_idx}\")\n",
        "                break\n",
        "            current_chunk_idx += len(chunks_in_pdf)\n",
        "\n",
        "    # Combine retrieved chunks\n",
        "    combined_text = ' '.join(retrieved_chunks)\n",
        "\n",
        "    # Refine the answer using a language model with a more structured prompt\n",
        "    prompt_template = (\n",
        "        \"Based on the following context, please answer the question. \"\n",
        "        \"If the answer is not available in the context, please state that you don't have enough information.\"\n",
        "        \"\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    )\n",
        "    formatted_prompt = prompt_template.format(context=combined_text, question=question)\n",
        "\n",
        "    llm_inputs = llm_tokenizer(formatted_prompt, return_tensors='pt', truncation=True, padding=True, max_length=1024)\n",
        "    llm_outputs = llm_model.generate(\n",
        "        **llm_inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    refined_answer = llm_tokenizer.decode(llm_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"Answer: {refined_answer}\\nSources: {sources}\"\n",
        "\n",
        "# Main function to tie everything together\n",
        "def main(folder_path, question, model_name, llm_model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "    llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    chunk_mapping = []\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        # Updated call to chunk_text with overlap_sentences\n",
        "        chunks = chunk_text(text, overlap_sentences=2) # Default overlap of 2 sentences\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    embeddings = create_embeddings(all_chunks, tokenizer, model)\n",
        "    index = index_embeddings(embeddings)\n",
        "\n",
        "    # Answer question\n",
        "    # Default temperature and max_new_tokens for main function if called directly\n",
        "    answer = answer_question(question, chunk_mapping, index, embeddings, tokenizer, model, llm_tokenizer, llm_model, temperature=0.5, max_new_tokens=150)\n",
        "    print(answer)"
      ],
      "metadata": {
        "id": "evg8kFes1H1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ca5762-6c7f-4b3a-d05c-17fcf394eb5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Implementation"
      ],
      "metadata": {
        "id": "W3sybnV4U9Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXZxcNhmOPbr",
        "outputId": "b938d88a-07b0-4dc2-aff5-55bb68f8180d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-6.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Collecting gradio-client==2.0.1 (from gradio)\n",
            "  Downloading gradio_client-2.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<13.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==2.0.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-6.1.0-py3-none-any.whl (23.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-2.0.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "Successfully installed gradio-6.1.0 gradio-client-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- Global Variables for Models and Indexed Data ---\n",
        "# Initialize these once when the script starts\n",
        "embedding_tokenizer = None\n",
        "embedding_model = None\n",
        "llm_tokenizer = None\n",
        "llm_model = None\n",
        "faiss_index = None\n",
        "all_chunks_mapping = None\n",
        "\n",
        "# Placeholder for loaded data. This will store (pdf_name, list_of_chunks)\n",
        "preprocessed_pdf_data = []\n",
        "\n",
        "# Assuming read_pdfs, chunk_text, create_embeddings, index_embeddings, and answer_question are available from previous cells.\n",
        "\n",
        "# Function to initialize models and preprocess PDFs\n",
        "def initialize_rag_system(folder_path, embedding_model_name, llm_model_name):\n",
        "    global embedding_tokenizer, embedding_model, llm_tokenizer, llm_model, faiss_index, all_chunks_mapping, preprocessed_pdf_data\n",
        "\n",
        "    print(\"Initializing RAG system...\")\n",
        "\n",
        "    # Load embedding model\n",
        "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "    embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
        "\n",
        "    # Load LLM\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "    llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    all_chunks_mapping = [] # This will hold the (pdf_name, list_of_chunks) for retrieval\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        all_chunks_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    # Only create embeddings once for all chunks\n",
        "    embeddings = create_embeddings(all_chunks, embedding_tokenizer, embedding_model)\n",
        "    faiss_index = index_embeddings(embeddings)\n",
        "\n",
        "    print(\"RAG system initialized.\")\n",
        "\n",
        "# Gradio Interface Functions\n",
        "def process_pdfs_and_answer_question(folder_path, question, embedding_model_name, llm_model_name, temperature, max_new_tokens):\n",
        "    global embedding_tokenizer, embedding_model, llm_tokenizer, llm_model, faiss_index, all_chunks_mapping\n",
        "\n",
        "    # Check if system is initialized, if not, initialize it\n",
        "    if embedding_tokenizer is None or embedding_model is None or llm_tokenizer is None or llm_model is None or faiss_index is None or all_chunks_mapping is None:\n",
        "        initialize_rag_system(folder_path, embedding_model_name, llm_model_name)\n",
        "\n",
        "    # Answer question using the globally loaded models and indexed data\n",
        "    # The temperature and max_new_tokens parameters are passed to the answer_question function\n",
        "    answer = answer_question(\n",
        "        question,\n",
        "        all_chunks_mapping, # Use the globally preprocessed chunks\n",
        "        faiss_index,\n",
        "        None, # Embeddings are not needed directly here, only the index is\n",
        "        embedding_tokenizer,\n",
        "        embedding_model,\n",
        "        llm_tokenizer,\n",
        "        llm_model,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# Create Gradio Interface\n",
        "# Note: The folder_path, embedding_model_name, and llm_model_name inputs will still be present\n",
        "# but the initialization will only happen once upon the first call or when the app starts if called before launch.\n",
        "iface = gr.Interface(\n",
        "    fn = process_pdfs_and_answer_question,\n",
        "    inputs = [\n",
        "        gr.Textbox(label = \"Path\", value = \"/content/drive/My Drive/PDFs/\", interactive = False), # Made non-interactive so user doesn't modify it\n",
        "        gr.Textbox(lines = 2, placeholder = \"Enter your question here...\", label = \"Question\"),\n",
        "        gr.Textbox(label = \"Embedding Model Name\", value = 'sentence-transformers/all-mpnet-base-v2', interactive = False), # Made non-interactive\n",
        "        gr.Textbox(label = \"LLM Model Name\", value = 'microsoft/GODEL-v1_1-large-seq2seq', interactive = False), # Made non-interactive\n",
        "        gr.Slider(minimum = 0.0, maximum = 1.0, value = 0.5, label = \"Temperature\"),\n",
        "        # Temperature control the randomness of the output. It essentially adjusts the probability distribution of the predicted tokens,\n",
        "        # influencing the diversity and creativity of the generated text.\n",
        "        # Low temperature value is useful when you want the model to generate precise and factual text.\n",
        "        # High temperature value leads to more creative and varied responses, but is more prone to making errors.\n",
        "        gr.Slider(minimum = 1, maximum = 256, value = 150, label = \"Max Tokens\")\n",
        "        # Max tokens is used for performance, cost control, and preventing crashes, achieved by implementing limits before sending text to the LLM.\n",
        "        # Higher max tokens is needed for detailed responses and the involvement of complex tasks.\n",
        "        # Lower max tokens is needed when speed is critical, cost is a concern, short, specific answers are sufficient, and when computational power must be conserved.\n",
        "    ],\n",
        "    outputs = [\n",
        "        gr.Textbox(lines = 10, label = \"Answer\")\n",
        "    ],\n",
        "    title = \"PDF Question Answering System\",\n",
        "    description = \"NOTE: Make sure to upload a PDF file into the specified folder path shown below. \\n Then, ask a question to get answers based on the content of the PDF file.\"\n",
        ")\n",
        "\n",
        "# Call initialization function once before launching Gradio, or ensure it's called on first invocation.\n",
        "# For a Colab environment, it might be better to call it explicitly here if you know the path upfront.\n",
        "initial_folder_path = \"/content/drive/My Drive/PDFs/\"\n",
        "initial_embedding_model = \"sentence-transformers/all-mpnet-base-v2\" # better than 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "# Had issues with the accuracy of the response when using sentence-transformers/all-MiniLM-L6-v2, so I opted for\n",
        "# sentence-transformers/all-mpnet-base-v2, where it has the higher accuracy of the response.\n",
        "initial_llm_model = 'microsoft/GODEL-v1_1-large-seq2seq'\n",
        "# microsoft/GODEL-v1_1-large-seq2seq is used as the LLM (Large Language Model) that takes the user's question and the\n",
        "# retrieved chunks of text (context) and synthesizes a coherent and relevant answer based on that information.\n",
        "# Its grounding capabilities are particularly beneficial for ensuring that the answers provided are directly supported by\n",
        "# your PDF documents.\n",
        "\n",
        "# This call ensures models and PDFs are preloaded before the Gradio interface is even interacted with.\n",
        "# It assumes the folder_path, embedding_model_name, and llm_model_name are fixed once Gradio is launched.\n",
        "initialize_rag_system(initial_folder_path, initial_embedding_model, initial_llm_model)\n",
        "\n",
        "iface.launch(share = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "RdoI9P4iNnTK",
        "outputId": "7369e24d-92b7-443b-922a-724f9616e80f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG system...\n",
            "RAG system initialized.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://63b0628112b9c28983.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://63b0628112b9c28983.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}